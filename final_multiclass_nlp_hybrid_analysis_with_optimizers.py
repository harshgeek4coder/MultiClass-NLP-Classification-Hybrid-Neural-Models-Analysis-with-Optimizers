# -*- coding: utf-8 -*-
"""Final - MultiClass NLP Hybrid Analysis with Optimizers

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13b6WqN7qkKcvtR-W0vMtNq-FMXNPpYwG

#Comparing Various Models for Multi-Class Classification for NLP Based Task

###Author - Harsh Sharma

#Importing Important Libraries :
"""

#Importing Important Libraries :
import csv
import tensorflow as tf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, LSTM, Dropout, Activation, Embedding, Bidirectional,Conv1D,MaxPool1D,MaxPooling1D,GlobalMaxPooling1D
from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
from mlxtend.plotting import plot_confusion_matrix
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split


import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
STOPWORDS = set(stopwords.words('english'))
from nltk.stem import PorterStemmer
import re
nltk.download('punkt')

!pip install prettytable
from prettytable import PrettyTable


import time

# Getting Dataset :
!wget --no-check-certificate \
    https://storage.googleapis.com/dataset-uploader/bbc/bbc-text.csv \
    -O /tmp/bbc-text.csv





#Converting Dataset to Dataframe :
articles = []
labels = []

with open("/tmp/bbc-text.csv", 'r') as csvfile:
    reader = csv.reader(csvfile, delimiter=',')
    next(reader)
    for row in reader:
        labels.append(row[0])
        article = row[1]
        for word in STOPWORDS:
            token = ' ' + word + ' '
            article = article.replace(token, ' ')
            article = article.replace(' ', ' ')
        articles.append(article)
print(len(labels))
print(len(articles))

raw_df=pd.DataFrame({"Text":articles,"Labels":labels})
raw_df

#Inspecting our Dataframe :
raw_df.info()

#Getting Number of Unique Classes in our dataset :
raw_df.Labels.unique(),print("\n Total number of Unique Target Classes : ",raw_df.Labels.nunique())



# Text cleanup :
stemmer = PorterStemmer()

REPLACE_BY_SPACE_RE = re.compile('[/(){}\[\]\|@,;]')
BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')
REMOVE_NUM = re.compile('[\d+]')
STOPWORDS = set(stopwords.words('english'))

def clean_text(text):
    """
    text: a string
    return: modified initial string
    """
    # lowercase text
    text = text.lower() 

    # replace REPLACE_BY_SPACE_RE symbols by space in text
    text = REPLACE_BY_SPACE_RE.sub(' ', text) 
    
    # Remove the XXXX values
    text = text.replace('x', '') 
    
    # Remove white space
    text = REMOVE_NUM.sub('', text)

    #  delete symbols which are in BAD_SYMBOLS_RE from text
    text = BAD_SYMBOLS_RE.sub('', text) 

    # delete stopwords from text
    text = ' '.join(word for word in text.split() if word not in STOPWORDS) 
    
    # removes any words composed of less than 2 or more than 21 letters
    text = ' '.join(word for word in text.split() if (len(word) >= 2 and len(word) <= 21))

    # Stemming the words
    text = ' '.join([stemmer.stem(word) for word in text.split()])
    
    return text

dataset=raw_df

dataset['Text']=dataset['Text'].apply(clean_text)
dataset['Text']

#Splitting raw data for Training and Testing :
text = dataset["Text"].values
labels = dataset['Labels'].values

X_train, y_train, X_test, y_test = train_test_split(text,labels, test_size = 0.20, random_state = 42)
print(X_train.shape,X_test.shape)
print(y_train.shape,y_test.shape)

#Reshaping Labels Input :
X_test=X_test.reshape(-1,1)
y_test=y_test.reshape(-1,1)

print(X_train.shape,X_test.shape)
print(y_train.shape,y_test.shape)

#Let's Fix Some Common Parameters :
# The maximum number of words to be used. (most frequent)
vocab_size = 50000

# Dimension of the dense embedding.
embedding_dim = 128

# Max number of words in each complaint.
max_length = 200

# Truncate and padding options
trunc_type = 'post'
padding_type = 'post'
oov_tok = '<OOV>'

# Tokenising , Converting Text To Sequences and Padding Our Data : 

tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')
tokenizer.fit_on_texts(X_train)
word_index = tokenizer.word_index
print('Found %s unique tokens.' % len(word_index))
word_index

# Converting into Text to sequences and padding :
train_seq = tokenizer.texts_to_sequences(X_train)
train_padded = pad_sequences(train_seq, maxlen=max_length, padding=padding_type, truncating=trunc_type)

validation_seq = tokenizer.texts_to_sequences(y_train)
validation_padded = pad_sequences(validation_seq, maxlen=max_length, padding=padding_type, truncating=trunc_type)

#Just an Example to see the raw sentance , sentance in sequences, sentance in sequences with padding:
X_train[3],train_seq[3],train_padded[3]

print('Shape of data tensor:', train_padded.shape)
print('Shape of data tensor:', validation_padded.shape)

#Using One Hot Enocder to Enocde our Multi class Labels  :
encode = OneHotEncoder()

training_labels = encode.fit_transform(X_test)
validation_labels = encode.transform(y_test)

print(train_padded.shape)
print(validation_labels.shape)
print(validation_padded.shape)
print(training_labels.shape)
print(type(train_padded))
print(type(validation_padded))
print(type(training_labels))
print(type(validation_labels))



# The labels must be converted to arrays
# Convert the labels to arrays
training_labels = training_labels.toarray()
validation_labels = validation_labels.toarray()

print(type(training_labels))
print(type(validation_labels))



"""## Basic Functions for Plotting and Evaluation :"""

def plot_graphs(history):
  plt.title('Loss VS Accuracy')
  plt.plot(history.history['loss'], label='train loss')
  plt.plot(history.history['val_loss'], label='test loss')
  plt.legend()
  
  plt.plot(history.history['accuracy'], label='train accuracy')
  plt.plot(history.history['val_accuracy'], label='test accuracy')
  plt.legend()
  plt.show();
  return None
def loss_graph(history):
  plt.title('Loss')
  plt.plot(history.history['loss'], label='train')
  plt.plot(history.history['val_loss'], label='test')
  plt.legend()
  plt.show();

  return None

def acc_graph(history):
  plt.title('Accuracy')
  plt.plot(history.history['accuracy'], label='train')
  plt.plot(history.history['val_accuracy'], label='test')
  plt.legend()
  plt.show();

  return None


def evaluate_preds(y_true, y_preds):
    """
    Performs evaluation comparison on y_true labels vs. y_pred labels
    on a classification.
    """
    accuracy = accuracy_score(y_true, y_preds)
    precision = precision_score(y_true, y_preds, average='micro')
    recall = recall_score(y_true, y_preds, average='micro')
    f1 = f1_score(y_true, y_preds, average='micro')
    metric_dict = {"accuracy": round(accuracy, 2),
                   "precision": round(precision, 2),
                   "recall": round(recall, 2),
                   "f1": round(f1, 2)}
    print(f"Acc: {accuracy * 100:.2f}%")
    print(f"Precision: {precision:.2f}")
    print(f"Recall: {recall:.2f}")
    print(f"F1 score: {f1:.2f}")
    
    return metric_dict

def model_eval(model,a,b,c,d):
  score=model.evaluate(a,b,verbose=0)
  score1=model.evaluate(c,d,verbose=0)
  print("\n")
  print("Model Loss on training data ",score[0])
  print("Model Accuracy on training data: ",score[1])
  print("Model Loss on validation data",score1[0])
  print("Model Accuracy on validation data: ",score1[1])
  print("\n")
  return score,score1

def model_eval(model,a,b,c,d):
  score=model.evaluate(a,b,verbose=0)
  score1=model.evaluate(c,d,verbose=0)
  print("\n")
  print("Model Loss on training data ",score[0])
  print("Model Accuracy on training data: ",score[1])
  print("Model Loss on validation data",score1[0])
  print("Model Accuracy on validation data: ",score1[1])
  print("\n")
  return score,score1

"""## Model Architectures :"""

#Let's Fix Some Common Parameters :
# The maximum number of words to be used. (most frequent)
vocab_size = 50000

# Dimension of the dense embedding.
embedding_dim = 128

# Max number of words in each complaint.
max_length = 200

# Truncate and padding options
trunc_type = 'post'
padding_type = 'post'
oov_tok = '<OOV>'

####TEST IN PROGRESS :







"""##Optimizers Used :
####Adam 
####AdaGrad
####SGD
####RMSPROP

1. Using Conv1D :
"""
#With Adam
tf.keras.backend.clear_session()
model = Sequential()
model.add(Embedding(vocab_size, embedding_dim, input_length=train_padded.shape[1]))

model.add(Conv1D(48, 5, activation='relu', padding='valid'))
model.add(GlobalMaxPooling1D())
model.add(Dropout(0.5))

model.add(Flatten())
model.add(Dropout(0.5))

model.add(Dense(5, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

epochs = 150
batch_size = 32
filepath="weights_best_cnn.hdf5"

checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max',save_weights_only=True)
start = time.perf_counter()
callbacks_list = [checkpoint,EarlyStopping(monitor='val_accuracy', mode='max', patience=10, verbose=1)]


history = model.fit(train_padded, training_labels, shuffle=True ,
                    epochs=epochs, batch_size=batch_size, 
                    callbacks=callbacks_list,validation_data=(validation_padded, validation_labels))
t1 = time.perf_counter() - start
print('Total time took for training %.3f seconds.' % t1)
hist1=history





plot_graphs(hist1),loss_graph(hist1),acc_graph(hist1)











# Now we make predictions using the test data to see how the model performs

predicted = model.predict(validation_padded)
evaluate_preds(np.argmax(validation_labels, axis=1), np.argmax(predicted, axis=1))

# Testing Random Text and Predicted Classes :

original_text=raw_df['Text']
text = original_text[18]
new_text = [clean_text(text)]
print(text)
print(new_text)
seq = tokenizer.texts_to_sequences(new_text)
padded = pad_sequences(seq, maxlen=max_length, padding=padding_type, truncating=trunc_type)
pred = model.predict(padded)
acc = model.predict_proba(padded)
predicted_label = encode.inverse_transform(pred)
print('')
print(f'Product category id: {np.argmax(pred[0])}')
print(f'Predicted label is: {predicted_label[0]}')
print(f'Accuracy score: { acc.max() * 100}')





# With AdaGrad
tf.keras.backend.clear_session()
model2 = Sequential()
model2.add(Embedding(vocab_size, embedding_dim, input_length=train_padded.shape[1]))

model2.add(Conv1D(48, 5, activation='relu', padding='valid'))
model2.add(GlobalMaxPooling1D())
model2.add(Dropout(0.5))

model2.add(Flatten())
model2.add(Dropout(0.5))

model2.add(Dense(5, activation='softmax'))

optim=tf.keras.optimizers.Adagrad(
    learning_rate=0.01
    
)
model2.compile(loss='categorical_crossentropy', optimizer=optim, metrics=['accuracy'])

epochs = 150
batch_size = 32
filepath="weights_best_cnn.hdf5"

checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max',save_weights_only=True)
start = time.perf_counter()
callbacks_list = [checkpoint,EarlyStopping(monitor='val_accuracy', mode='max', patience=10, verbose=1)]


history = model2.fit(train_padded, training_labels, shuffle=True ,
                    epochs=epochs, batch_size=batch_size, 
                    callbacks=callbacks_list,validation_data=(validation_padded, validation_labels))
t2 = time.perf_counter() - start
print('Total time took for training %.3f seconds.' % t2)
hist2=history


plot_graphs(hist2),loss_graph(hist2),acc_graph(hist2)

# Now we make predictions using the test data to see how the model performs

predicted = model2.predict(validation_padded)
evaluate_preds(np.argmax(validation_labels, axis=1), np.argmax(predicted, axis=1))

# With SGD
tf.keras.backend.clear_session()
model3 = Sequential()
model3.add(Embedding(vocab_size, embedding_dim, input_length=train_padded.shape[1]))

model3.add(Conv1D(48, 5, activation='relu', padding='valid'))
model3.add(GlobalMaxPooling1D())
model3.add(Dropout(0.5))

model3.add(Flatten())
model3.add(Dropout(0.5))

model3.add(Dense(5, activation='softmax'))

optim=tf.keras.optimizers.SGD(
    learning_rate=0.01, momentum=0.5, nesterov=True, name="SGD"
)
model3.compile(loss='categorical_crossentropy', optimizer=optim, metrics=['accuracy'])

epochs = 150
batch_size = 32
filepath="weights_best_cnn.hdf5"

checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max',save_weights_only=True)
start = time.perf_counter()
callbacks_list = [checkpoint,EarlyStopping(monitor='val_accuracy', mode='max', patience=10, verbose=1)]


history = model3.fit(train_padded, training_labels, shuffle=True ,
                    epochs=epochs, batch_size=batch_size, 
                    callbacks=callbacks_list,validation_data=(validation_padded, validation_labels))
t3 = time.perf_counter() - start
print('Total time took for training %.3f seconds.' % t3)
hist3=history


plot_graphs(hist3),loss_graph(hist3),acc_graph(hist3)

# Now we make predictions using the test data to see how the model performs

predicted = model3.predict(validation_padded)
evaluate_preds(np.argmax(validation_labels, axis=1), np.argmax(predicted, axis=1))

# With RMSPROP
tf.keras.backend.clear_session()
model4 = Sequential()
model4.add(Embedding(vocab_size, embedding_dim, input_length=train_padded.shape[1]))

model4.add(Conv1D(48, 5, activation='relu', padding='valid'))
model4.add(GlobalMaxPooling1D())
model4.add(Dropout(0.5))

model4.add(Flatten())
model4.add(Dropout(0.5))

model4.add(Dense(5, activation='softmax'))

optim=tf.keras.optimizers.RMSprop(
    learning_rate=0.01
    
)
model4.compile(loss='categorical_crossentropy', optimizer=optim, metrics=['accuracy'])

epochs = 150
batch_size = 32
filepath="weights_best_cnn.hdf5"

checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max',save_weights_only=True)
start = time.perf_counter()
callbacks_list = [checkpoint,EarlyStopping(monitor='val_accuracy', mode='max', patience=10, verbose=1)]


history = model4.fit(train_padded, training_labels, shuffle=True ,
                    epochs=epochs, batch_size=batch_size, 
                    callbacks=callbacks_list,validation_data=(validation_padded, validation_labels))
t4 = time.perf_counter() - start
print('Total time took for training %.3f seconds.' % t4)
hist4=history


plot_graphs(hist4),loss_graph(hist4),acc_graph(hist4)

# Now we make predictions using the test data to see how the model performs

predicted = model4.predict(validation_padded)
evaluate_preds(np.argmax(validation_labels, axis=1), np.argmax(predicted, axis=1))

# With Adadelta
tf.keras.backend.clear_session()

model5 = Sequential()
model5.add(Embedding(vocab_size, embedding_dim, input_length=train_padded.shape[1]))

model5.add(Conv1D(48, 5, activation='relu', padding='valid'))
model5.add(GlobalMaxPooling1D())
model5.add(Dropout(0.5))

model5.add(Flatten())
model5.add(Dropout(0.5))

model5.add(Dense(5, activation='softmax'))

optim=tf.keras.optimizers.Adadelta(
    learning_rate=0.01
)
model5.compile(loss='categorical_crossentropy', optimizer=optim, metrics=['accuracy'])

epochs = 150
batch_size = 32
filepath="weights_best_cnn.hdf5"

checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max',save_weights_only=True)
start = time.perf_counter()
callbacks_list = [checkpoint,EarlyStopping(monitor='val_accuracy', mode='max', patience=10, verbose=1)]


history = model5.fit(train_padded, training_labels, shuffle=True ,
                    epochs=epochs, batch_size=batch_size, 
                    callbacks=callbacks_list,validation_data=(validation_padded, validation_labels))
t5 = time.perf_counter() - start
print('Total time took for training %.3f seconds.' % t5)
hist5=history


plot_graphs(hist5),loss_graph(hist5),acc_graph(hist5)

# Now we make predictions using the test data to see how the model performs

predicted = model5.predict(validation_padded)
evaluate_preds(np.argmax(validation_labels, axis=1), np.argmax(predicted, axis=1))

"""2.Using LSTMs :"""
#With Adam
tf.keras.backend.clear_session()
model6 = Sequential()

model6.add(Embedding(vocab_size, embedding_dim,input_length=train_padded.shape[1]))
model6.add(Dropout(0.5))
model6.add(LSTM(embedding_dim))
model6.add(Dense(5, activation='softmax'))

model6.summary()

model6.compile(
    loss='categorical_crossentropy',
    optimizer='adam',
    metrics=['accuracy'],
)

epochs = 150
batch_size = 32
filepath="weights_best_lstms.hdf5"

checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max',save_weights_only=True)
start = time.perf_counter()
callbacks_list = [checkpoint,EarlyStopping(monitor='val_accuracy', mode='max', patience=10, verbose=1)]


history = model6.fit(train_padded, training_labels, shuffle=True ,
                    epochs=epochs, batch_size=batch_size, 
                    callbacks=callbacks_list,validation_data=(validation_padded, validation_labels))
t6 = time.perf_counter() - start
print('Total time took for training %.3f seconds.' % t6)
hist6=history

plot_graphs(hist6),loss_graph(hist6),acc_graph(hist6)

predicted = model6.predict(validation_padded)
evaluate_preds(np.argmax(validation_labels, axis=1), np.argmax(predicted, axis=1))



#With Adagrad :
tf.keras.backend.clear_session()
model7 = Sequential()

model7.add(Embedding(vocab_size, embedding_dim,input_length=train_padded.shape[1]))
model7.add(Dropout(0.5))
model7.add(LSTM(embedding_dim))
model7.add(Dense(5, activation='softmax'))

model7.summary()
optim=tf.keras.optimizers.Adagrad(
    learning_rate=0.01
)
model7.compile(
    loss='categorical_crossentropy',
    optimizer=optim,
    metrics=['accuracy'],
)

epochs = 150
batch_size = 32
filepath="weights_best_lstms.hdf5"

checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max',save_weights_only=True)
start = time.perf_counter()
callbacks_list = [checkpoint,EarlyStopping(monitor='val_accuracy', mode='max', patience=10, verbose=1)]


history = model7.fit(train_padded, training_labels, shuffle=True ,
                    epochs=epochs, batch_size=batch_size, 
                    callbacks=callbacks_list,validation_data=(validation_padded, validation_labels))
t7 = time.perf_counter() - start
print('Total time took for training %.3f seconds.' % t7)
hist7=history

plot_graphs(hist7),loss_graph(hist7),acc_graph(hist7)

# Now we make predictions using the test data to see how the model performs

predicted = model7.predict(validation_padded)
evaluate_preds(np.argmax(validation_labels, axis=1), np.argmax(predicted, axis=1))

#With SGD :
tf.keras.backend.clear_session()
model8 = Sequential()

model8.add(Embedding(vocab_size, embedding_dim,input_length=train_padded.shape[1]))
model8.add(Dropout(0.5))
model8.add(LSTM(embedding_dim))
model8.add(Dense(5, activation='softmax'))

model8.summary()
optim=tf.keras.optimizers.SGD(
    learning_rate=0.01, momentum=0.5, nesterov=True, name="SGD"
)
model8.compile(
    loss='categorical_crossentropy',
    optimizer=optim,
    metrics=['accuracy'],
)

epochs = 150
batch_size = 32
filepath="weights_best_lstms.hdf5"

checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max',save_weights_only=True)
start = time.perf_counter()
callbacks_list = [checkpoint,EarlyStopping(monitor='val_accuracy', mode='max', patience=10, verbose=1)]


history = model8.fit(train_padded, training_labels, shuffle=True ,
                    epochs=epochs, batch_size=batch_size, 
                    callbacks=callbacks_list,validation_data=(validation_padded, validation_labels))
t8 = time.perf_counter() - start
print('Total time took for training %.3f seconds.' % t8)
hist8=history

plot_graphs(hist8),loss_graph(hist8),acc_graph(hist8)

# Now we make predictions using the test data to see how the model performs

predicted = model8.predict(validation_padded)
evaluate_preds(np.argmax(validation_labels, axis=1), np.argmax(predicted, axis=1))

#With RMSProp :
tf.keras.backend.clear_session()
model9 = Sequential()

model9.add(Embedding(vocab_size, embedding_dim,input_length=train_padded.shape[1]))
model9.add(Dropout(0.5))
model9.add(LSTM(embedding_dim))
model9.add(Dense(5, activation='softmax'))

model9.summary()
optim=tf.keras.optimizers.RMSprop(
    learning_rate=0.01
)
model9.compile(
    loss='categorical_crossentropy',
    optimizer=optim,
    metrics=['accuracy'],
)

epochs = 150
batch_size = 32
filepath="weights_best_lstms.hdf5"

checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max',save_weights_only=True)
start = time.perf_counter()
callbacks_list = [checkpoint,EarlyStopping(monitor='val_accuracy', mode='max', patience=10, verbose=1)]


history = model9.fit(train_padded, training_labels, shuffle=True ,
                    epochs=epochs, batch_size=batch_size, 
                    callbacks=callbacks_list,validation_data=(validation_padded, validation_labels))
t9 = time.perf_counter() - start
print('Total time took for training %.3f seconds.' % t9)
hist9=history

plot_graphs(hist9),loss_graph(hist9),acc_graph(hist9)

# Now we make predictions using the test data to see how the model performs

predicted = model9.predict(validation_padded)
evaluate_preds(np.argmax(validation_labels, axis=1), np.argmax(predicted, axis=1))

#With Adadelta :
tf.keras.backend.clear_session()
model10 = Sequential()

model10.add(Embedding(vocab_size, embedding_dim,input_length=train_padded.shape[1]))
model10.add(Dropout(0.5))
model10.add(LSTM(embedding_dim))
model10.add(Dense(5, activation='softmax'))

model10.summary()
optim=tf.keras.optimizers.Adadelta(
    learning_rate=0.01)
model10.compile(
    loss='categorical_crossentropy',
    optimizer=optim,
    metrics=['accuracy'],
)

epochs = 150
batch_size = 32
filepath="weights_best_lstms.hdf5"

checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max',save_weights_only=True)
start = time.perf_counter()
callbacks_list = [checkpoint,EarlyStopping(monitor='val_accuracy', mode='max', patience=10, verbose=1)]


history = model10.fit(train_padded, training_labels, shuffle=True ,
                    epochs=epochs, batch_size=batch_size, 
                    callbacks=callbacks_list,validation_data=(validation_padded, validation_labels))
t10 = time.perf_counter() - start
print('Total time took for training %.3f seconds.' % t10)
hist10=history

plot_graphs(hist10),loss_graph(hist10),acc_graph(hist10)

# Now we make predictions using the test data to see how the model performs

predicted = model10.predict(validation_padded)
evaluate_preds(np.argmax(validation_labels, axis=1), np.argmax(predicted, axis=1))

"""3.Using Bi-Directional LSTMs :"""

tf.keras.backend.clear_session()
model11 = Sequential()

model11.add(Embedding(vocab_size, embedding_dim,input_length=train_padded.shape[1]))
model11.add(Dropout(0.5))
model11.add(Bidirectional(LSTM(embedding_dim)))
model11.add(Dense(5, activation='softmax'))

model11.summary()

model11.compile(
    loss='categorical_crossentropy',
    optimizer='adam',
    metrics=['accuracy'],
)

epochs = 150
batch_size = 32
filepath="weights_best_bi_lstms.hdf5"

checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max',save_weights_only=True)
start = time.perf_counter()
callbacks_list = [checkpoint,EarlyStopping(monitor='val_accuracy', mode='max', patience=10, verbose=1)]


history = model11.fit(train_padded, training_labels, shuffle=True ,
                    epochs=epochs, batch_size=batch_size, 
                    callbacks=callbacks_list,validation_data=(validation_padded, validation_labels))
t11 = time.perf_counter() - start
print('Total time took for training %.3f seconds.' % t11)
hist11=history



plot_graphs(hist11),loss_graph(hist11),acc_graph(hist11)

predicted = model11.predict(validation_padded)
evaluate_preds(np.argmax(validation_labels, axis=1), np.argmax(predicted, axis=1))





#with Adagrad
tf.keras.backend.clear_session()
model12 = Sequential()

model12.add(Embedding(vocab_size, embedding_dim,input_length=train_padded.shape[1]))
model12.add(Dropout(0.5))
model12.add(Bidirectional(LSTM(embedding_dim)))
model12.add(Dense(5, activation='softmax'))

model12.summary()
optim=tf.keras.optimizers.Adagrad(
    learning_rate=0.01
)
model12.compile(
    loss='categorical_crossentropy',
    optimizer=optim,
    metrics=['accuracy'],
)

epochs = 150
batch_size = 32
filepath="weights_best_bi_lstms.hdf5"

checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max',save_weights_only=True)
start = time.perf_counter()
callbacks_list = [checkpoint,EarlyStopping(monitor='val_accuracy', mode='max', patience=10, verbose=1)]


history = model12.fit(train_padded, training_labels, shuffle=True ,
                    epochs=epochs, batch_size=batch_size, 
                    callbacks=callbacks_list,validation_data=(validation_padded, validation_labels))
t12 = time.perf_counter() - start
print('Total time took for training %.3f seconds.' % t12)
hist12=history

plot_graphs(hist12),loss_graph(hist12),acc_graph(hist12)

# Now we make predictions using the test data to see how the model performs

predicted = model12.predict(validation_padded)
evaluate_preds(np.argmax(validation_labels, axis=1), np.argmax(predicted, axis=1))

#with SGD
tf.keras.backend.clear_session()
model13 = Sequential()

model13.add(Embedding(vocab_size, embedding_dim,input_length=train_padded.shape[1]))
model13.add(Dropout(0.5))
model13.add(Bidirectional(LSTM(embedding_dim)))
model13.add(Dense(5, activation='softmax'))

model13.summary()
optim=tf.keras.optimizers.SGD(
    learning_rate=0.01, momentum=0.5, nesterov=True, name="SGD"
)
model13.compile(
    loss='categorical_crossentropy',
    optimizer=optim,
    metrics=['accuracy'],
)

epochs = 150
batch_size = 32
filepath="weights_best_bi_lstms.hdf5"

checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max',save_weights_only=True)
start = time.perf_counter()
callbacks_list = [checkpoint,EarlyStopping(monitor='val_accuracy', mode='max', patience=10, verbose=1)]


history = model13.fit(train_padded, training_labels, shuffle=True ,
                    epochs=epochs, batch_size=batch_size, 
                    callbacks=callbacks_list,validation_data=(validation_padded, validation_labels))
t13 = time.perf_counter() - start
print('Total time took for training %.3f seconds.' % t13)
hist13=history

plot_graphs(hist13),loss_graph(hist13),acc_graph(hist13)

# Now we make predictions using the test data to see how the model performs

predicted = model13.predict(validation_padded)
evaluate_preds(np.argmax(validation_labels, axis=1), np.argmax(predicted, axis=1))

#with RMSProp
tf.keras.backend.clear_session()
model14 = Sequential()

model14.add(Embedding(vocab_size, embedding_dim,input_length=train_padded.shape[1]))
model14.add(Dropout(0.5))
model14.add(Bidirectional(LSTM(embedding_dim)))
model14.add(Dense(5, activation='softmax'))

model14.summary()
optim=tf.keras.optimizers.RMSprop(
    learning_rate=0.01
)
model14.compile(
    loss='categorical_crossentropy',
    optimizer=optim,
    metrics=['accuracy'],
)

epochs = 150
batch_size = 32
filepath="weights_best_bi_lstms.hdf5"

checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max',save_weights_only=True)
start = time.perf_counter()
callbacks_list = [checkpoint,EarlyStopping(monitor='val_accuracy', mode='max', patience=10, verbose=1)]


history = model14.fit(train_padded, training_labels, shuffle=True ,
                    epochs=epochs, batch_size=batch_size, 
                    callbacks=callbacks_list,validation_data=(validation_padded, validation_labels))
t14 = time.perf_counter() - start
print('Total time took for training %.3f seconds.' % t14)
hist14=history

plot_graphs(hist14),loss_graph(hist14),acc_graph(hist14)

# Now we make predictions using the test data to see how the model performs

predicted = model14.predict(validation_padded)
evaluate_preds(np.argmax(validation_labels, axis=1), np.argmax(predicted, axis=1))

#with Adadelta
tf.keras.backend.clear_session()
model15 = Sequential()

model15.add(Embedding(vocab_size, embedding_dim,input_length=train_padded.shape[1]))
model15.add(Dropout(0.5))
model15.add(Bidirectional(LSTM(embedding_dim)))
model15.add(Dense(5, activation='softmax'))

model15.summary()
optim=tf.keras.optimizers.Adadelta(
    learning_rate=0.01
)
model15.compile(
    loss='categorical_crossentropy',
    optimizer=optim,
    metrics=['accuracy'],
)

epochs = 150
batch_size = 32
filepath="weights_best_bi_lstms.hdf5"

checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max',save_weights_only=True)
start = time.perf_counter()
callbacks_list = [checkpoint,EarlyStopping(monitor='val_accuracy', mode='max', patience=10, verbose=1)]


history = model15.fit(train_padded, training_labels, shuffle=True ,
                    epochs=epochs, batch_size=batch_size, 
                    callbacks=callbacks_list,validation_data=(validation_padded, validation_labels))
t15 = time.perf_counter() - start
print('Total time took for training %.3f seconds.' % t15)
hist15=history

plot_graphs(hist15),loss_graph(hist15),acc_graph(hist15)

# Now we make predictions using the test data to see how the model performs

predicted = model15.predict(validation_padded)
evaluate_preds(np.argmax(validation_labels, axis=1), np.argmax(predicted, axis=1))

"""4.Using CNN1D + LSTMs :"""
#With CNN 1-D + LSTMs
#With Adam
tf.keras.backend.clear_session()
model16 = Sequential()

model16.add(Embedding(vocab_size, embedding_dim,input_length=train_padded.shape[1]))
model16.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))
model16.add(MaxPooling1D(pool_size=2))
model16.add(LSTM(embedding_dim))
model16.add(Dense(5, activation='softmax'))

model16.summary()

model16.compile(
    loss='categorical_crossentropy',
    optimizer='adam',
    metrics=['accuracy'],
)

epochs = 150
batch_size = 32
filepath="weights_best_lstms_n_cnn.hdf5"

checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max',save_weights_only=True)
start = time.perf_counter()
callbacks_list = [checkpoint,EarlyStopping(monitor='val_accuracy', mode='max', patience=10, verbose=1)]


history = model16.fit(train_padded, training_labels, shuffle=True ,
                    epochs=epochs, batch_size=batch_size, 
                    callbacks=callbacks_list,validation_data=(validation_padded, validation_labels))
t16 = time.perf_counter() - start
print('Total time took for training %.3f seconds.' % t16)
hist16=history

plot_graphs(hist16),loss_graph(hist16),acc_graph(hist16)

predicted = model16.predict(validation_padded)
evaluate_preds(np.argmax(validation_labels, axis=1), np.argmax(predicted, axis=1))



#With AdaGrad
tf.keras.backend.clear_session()
model17 = Sequential()

model17.add(Embedding(vocab_size, embedding_dim,input_length=train_padded.shape[1]))
model17.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))
model17.add(MaxPooling1D(pool_size=2))
model17.add(LSTM(embedding_dim))
model17.add(Dense(5, activation='softmax'))

model17.summary()
optim=tf.keras.optimizers.Adagrad(
    learning_rate=0.01
)
model17.compile(
    loss='categorical_crossentropy',
    optimizer=optim,
    metrics=['accuracy'],
)

epochs = 150
batch_size = 32
filepath="weights_best_lstms_n_cnn.hdf5"

checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max',save_weights_only=True)
start = time.perf_counter()
callbacks_list = [checkpoint,EarlyStopping(monitor='val_accuracy', mode='max', patience=10, verbose=1)]


history = model17.fit(train_padded, training_labels, shuffle=True ,
                    epochs=epochs, batch_size=batch_size, 
                    callbacks=callbacks_list,validation_data=(validation_padded, validation_labels))
t17 = time.perf_counter() - start
print('Total time took for training %.3f seconds.' % t17)
hist17=history

plot_graphs(hist17),loss_graph(hist17),acc_graph(hist17)

# Now we make predictions using the test data to see how the model performs

predicted = model17.predict(validation_padded)
evaluate_preds(np.argmax(validation_labels, axis=1), np.argmax(predicted, axis=1))

#With SGD
tf.keras.backend.clear_session()
model18 = Sequential()

model18.add(Embedding(vocab_size, embedding_dim,input_length=train_padded.shape[1]))
model18.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))
model18.add(MaxPooling1D(pool_size=2))
model18.add(LSTM(embedding_dim))
model18.add(Dense(5, activation='softmax'))

model18.summary()
optim=tf.keras.optimizers.SGD(
    learning_rate=0.01, momentum=0.5, nesterov=True, name="SGD"
)
model18.compile(
    loss='categorical_crossentropy',
    optimizer=optim,
    metrics=['accuracy'],
)

epochs = 150
batch_size = 32
filepath="weights_best_lstms_n_cnn.hdf5"

checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max',save_weights_only=True)
start = time.perf_counter()
callbacks_list = [checkpoint,EarlyStopping(monitor='val_accuracy', mode='max', patience=10, verbose=1)]


history = model18.fit(train_padded, training_labels, shuffle=True ,
                    epochs=epochs, batch_size=batch_size, 
                    callbacks=callbacks_list,validation_data=(validation_padded, validation_labels))
t18 = time.perf_counter() - start
print('Total time took for training %.3f seconds.' % t18)
hist18=history

plot_graphs(hist18),loss_graph(hist18),acc_graph(hist18)

# Now we make predictions using the test data to see how the model performs

predicted = model18.predict(validation_padded)
evaluate_preds(np.argmax(validation_labels, axis=1), np.argmax(predicted, axis=1))

#With RMSprop
tf.keras.backend.clear_session()
model19 = Sequential()

model19.add(Embedding(vocab_size, embedding_dim,input_length=train_padded.shape[1]))
model19.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))
model19.add(MaxPooling1D(pool_size=2))
model19.add(LSTM(embedding_dim))
model19.add(Dense(5, activation='softmax'))

model19.summary()
optim=tf.keras.optimizers.RMSprop(
    learning_rate=0.01
)
model19.compile(
    loss='categorical_crossentropy',
    optimizer=optim,
    metrics=['accuracy'],
)

epochs = 150
batch_size = 32
filepath="weights_best_lstms_n_cnn.hdf5"

checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max',save_weights_only=True)
start = time.perf_counter()
callbacks_list = [checkpoint,EarlyStopping(monitor='val_accuracy', mode='max', patience=10, verbose=1)]


history = model19.fit(train_padded, training_labels, shuffle=True ,
                    epochs=epochs, batch_size=batch_size, 
                    callbacks=callbacks_list,validation_data=(validation_padded, validation_labels))
t19 = time.perf_counter() - start
print('Total time took for training %.3f seconds.' % t19)
hist19=history

plot_graphs(hist19),loss_graph(hist19),acc_graph(hist19)

# Now we make predictions using the test data to see how the model performs

predicted = model19.predict(validation_padded)
evaluate_preds(np.argmax(validation_labels, axis=1), np.argmax(predicted, axis=1))

#With AdaDelta
tf.keras.backend.clear_session()
model20 = Sequential()

model20.add(Embedding(vocab_size, embedding_dim,input_length=train_padded.shape[1]))
model20.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))
model20.add(MaxPooling1D(pool_size=2))
model20.add(LSTM(embedding_dim))
model20.add(Dense(5, activation='softmax'))

model20.summary()
optim=tf.keras.optimizers.Adadelta(
    learning_rate=0.01
)
model20.compile(
    loss='categorical_crossentropy',
    optimizer=optim,
    metrics=['accuracy'],
)

epochs = 150
batch_size = 32
filepath="weights_best_lstms_n_cnn.hdf5"

checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max',save_weights_only=True)
start = time.perf_counter()
callbacks_list = [checkpoint,EarlyStopping(monitor='val_accuracy', mode='max', patience=10, verbose=1)]


history = model20.fit(train_padded, training_labels, shuffle=True ,
                    epochs=epochs, batch_size=batch_size, 
                    callbacks=callbacks_list,validation_data=(validation_padded, validation_labels))
t20 = time.perf_counter() - start
print('Total time took for training %.3f seconds.' % t20)
hist20=history

plot_graphs(hist20),loss_graph(hist20),acc_graph(hist20)

# Now we make predictions using the test data to see how the model performs

predicted = model20.predict(validation_padded)
evaluate_preds(np.argmax(validation_labels, axis=1), np.argmax(predicted, axis=1))

"""5.Using Bi_LSTMs + CNN1D :"""
#Bi_directional_LSTMs + CNN1D :
#With Adam
tf.keras.backend.clear_session()
model21 = Sequential()

model21.add(Embedding(vocab_size, embedding_dim,input_length=train_padded.shape[1]))
model21.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))
model21.add(MaxPooling1D(pool_size=2))
model21.add(Bidirectional(LSTM(embedding_dim)))
model21.add(Dense(5, activation='softmax'))

model21.summary()

model21.compile(
    loss='categorical_crossentropy',
    optimizer='adam',
    metrics=['accuracy'],
)

epochs = 150
batch_size = 32
filepath="weights_best_bi_lstms_n_cnn.hdf5"

checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max',save_weights_only=True)
start = time.perf_counter()
callbacks_list = [checkpoint,EarlyStopping(monitor='val_accuracy', mode='max', patience=10, verbose=1)]


history = model21.fit(train_padded, training_labels, shuffle=True ,
                    epochs=epochs, batch_size=batch_size, 
                    callbacks=callbacks_list,validation_data=(validation_padded, validation_labels))
t21 = time.perf_counter() - start
print('Total time took for training %.3f seconds.' % t21)
hist21=history

plot_graphs(hist21),loss_graph(hist21),acc_graph(hist21)

predicted = model21.predict(validation_padded)
evaluate_preds(np.argmax(validation_labels, axis=1), np.argmax(predicted, axis=1))



#With AdaGrad
tf.keras.backend.clear_session()
model22 = Sequential()

model22.add(Embedding(vocab_size, embedding_dim,input_length=train_padded.shape[1]))
model22.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))
model22.add(MaxPooling1D(pool_size=2))
model22.add(Bidirectional(LSTM(embedding_dim)))
model22.add(Dense(5, activation='softmax'))

model22.summary()
optim=tf.keras.optimizers.Adagrad(
    learning_rate=0.01
)
model22.compile(
    loss='categorical_crossentropy',
    optimizer=optim,
    metrics=['accuracy'],
)

epochs = 150
batch_size = 32
filepath="weights_best_bi_lstms_n_cnn.hdf5"

checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max',save_weights_only=True)
start = time.perf_counter()
callbacks_list = [checkpoint,EarlyStopping(monitor='val_accuracy', mode='max', patience=10, verbose=1)]


history = model22.fit(train_padded, training_labels, shuffle=True ,
                    epochs=epochs, batch_size=batch_size, 
                    callbacks=callbacks_list,validation_data=(validation_padded, validation_labels))
t22 = time.perf_counter() - start
print('Total time took for training %.3f seconds.' % t22)
hist22=history

plot_graphs(hist22),loss_graph(hist22),acc_graph(hist22)

# Now we make predictions using the test data to see how the model performs

predicted = model22.predict(validation_padded)
evaluate_preds(np.argmax(validation_labels, axis=1), np.argmax(predicted, axis=1))

#With SGD
tf.keras.backend.clear_session()
model23 = Sequential()

model23.add(Embedding(vocab_size, embedding_dim,input_length=train_padded.shape[1]))
model23.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))
model23.add(MaxPooling1D(pool_size=2))
model23.add(Bidirectional(LSTM(embedding_dim)))
model23.add(Dense(5, activation='softmax'))

model23.summary()
optim=tf.keras.optimizers.SGD(
    learning_rate=0.01, momentum=0.5, nesterov=True, name="SGD"
)
model23.compile(
    loss='categorical_crossentropy',
    optimizer=optim,
    metrics=['accuracy'],
)

epochs = 150
batch_size = 32
filepath="weights_best_bi_lstms_n_cnn.hdf5"

checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max',save_weights_only=True)
start = time.perf_counter()
callbacks_list = [checkpoint,EarlyStopping(monitor='val_accuracy', mode='max', patience=10, verbose=1)]


history = model23.fit(train_padded, training_labels, shuffle=True ,
                    epochs=epochs, batch_size=batch_size, 
                    callbacks=callbacks_list,validation_data=(validation_padded, validation_labels))
t23 = time.perf_counter() - start
print('Total time took for training %.3f seconds.' % t23)
hist23=history

plot_graphs(hist23),loss_graph(hist23),acc_graph(hist23)

# Now we make predictions using the test data to see how the model performs

predicted = model23.predict(validation_padded)
evaluate_preds(np.argmax(validation_labels, axis=1), np.argmax(predicted, axis=1))

#With RMSProp
tf.keras.backend.clear_session()
model24 = Sequential()

model24.add(Embedding(vocab_size, embedding_dim,input_length=train_padded.shape[1]))
model24.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))
model24.add(MaxPooling1D(pool_size=2))
model24.add(Bidirectional(LSTM(embedding_dim)))
model24.add(Dense(5, activation='softmax'))

model24.summary()
optim=tf.keras.optimizers.RMSprop(
    learning_rate=0.01
)
model24.compile(
    loss='categorical_crossentropy',
    optimizer=optim,
    metrics=['accuracy'],
)

epochs = 150
batch_size = 32
filepath="weights_best_bi_lstms_n_cnn.hdf5"

checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max',save_weights_only=True)
start = time.perf_counter()
callbacks_list = [checkpoint,EarlyStopping(monitor='val_accuracy', mode='max', patience=10, verbose=1)]


history = model24.fit(train_padded, training_labels, shuffle=True ,
                    epochs=epochs, batch_size=batch_size, 
                    callbacks=callbacks_list,validation_data=(validation_padded, validation_labels))
t24 = time.perf_counter() - start
print('Total time took for training %.3f seconds.' % t24)
hist24=history

plot_graphs(hist24),loss_graph(hist24),acc_graph(hist24)

# Now we make predictions using the test data to see how the model performs

predicted = model24.predict(validation_padded)
evaluate_preds(np.argmax(validation_labels, axis=1), np.argmax(predicted, axis=1))

#With Adadelta
tf.keras.backend.clear_session()
model25 = Sequential()

model25.add(Embedding(vocab_size, embedding_dim,input_length=train_padded.shape[1]))
model25.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))
model25.add(MaxPooling1D(pool_size=2))
model25.add(Bidirectional(LSTM(embedding_dim)))
model25.add(Dense(5, activation='softmax'))

model25.summary()
optim=tf.keras.optimizers.Adadelta(
    learning_rate=0.01
)
model25.compile(
    loss='categorical_crossentropy',
    optimizer=optim,
    metrics=['accuracy'],
)

epochs = 150
batch_size = 32
filepath="weights_best_bi_lstms_n_cnn.hdf5"

checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max',save_weights_only=True)
start = time.perf_counter()
callbacks_list = [checkpoint,EarlyStopping(monitor='val_accuracy', mode='max', patience=10, verbose=1)]


history = model25.fit(train_padded, training_labels, shuffle=True ,
                    epochs=epochs, batch_size=batch_size, 
                    callbacks=callbacks_list,validation_data=(validation_padded, validation_labels))
t25 = time.perf_counter() - start
print('Total time took for training %.3f seconds.' % t25)
hist25=history

plot_graphs(hist25),loss_graph(hist25),acc_graph(hist25)

# Now we make predictions using the test data to see how the model performs

predicted = model25.predict(validation_padded)
evaluate_preds(np.argmax(validation_labels, axis=1), np.argmax(predicted, axis=1))

"""## Overall Evaluation of all Models"""



print("Final Performance Of All Models : \n")
print("Model 1 - CNN-1D \n")





print("With Adam : \n")
mod1=model_eval(model,train_padded,training_labels,validation_padded,validation_labels)
print("With AdaGrad : \n")
mod2=model_eval(model2,train_padded,training_labels,validation_padded,validation_labels)
print("With SGD : \n")
mod3=model_eval(model3,train_padded,training_labels,validation_padded,validation_labels)
print("With RMSProp : \n")
mod4=model_eval(model4,train_padded,training_labels,validation_padded,validation_labels)
print("With AdaDelta : \n")
mod5=model_eval(model5,train_padded,training_labels,validation_padded,validation_labels)

print("Model 2 - LSTM \n")

print("With Adam : \n")
mod6=model_eval(model6,train_padded,training_labels,validation_padded,validation_labels)
print("With AdaGrad : \n")
mod7=model_eval(model7,train_padded,training_labels,validation_padded,validation_labels)
print("With SGD : \n")
mod8=model_eval(model8,train_padded,training_labels,validation_padded,validation_labels)
print("With RMSProp : \n")
mod9=model_eval(model9,train_padded,training_labels,validation_padded,validation_labels)
print("With AdaDelta : \n")
mod10=model_eval(model10,train_padded,training_labels,validation_padded,validation_labels)


print("Model 3 - BiDirectional LSTM \n")
mod11=model_eval(model11,train_padded,training_labels,validation_padded,validation_labels)
print("With AdaGrad : \n")
mod12=model_eval(model12,train_padded,training_labels,validation_padded,validation_labels)
print("With SGD : \n")
mod13=model_eval(model13,train_padded,training_labels,validation_padded,validation_labels)
print("With RMSProp : \n")
mod14=model_eval(model14,train_padded,training_labels,validation_padded,validation_labels)
print("With AdaDelta : \n")
mod15=model_eval(model15,train_padded,training_labels,validation_padded,validation_labels)

print("Model 4 - CNN 1-D + LSTM \n")

print("With Adam : \n")
mod16=model_eval(model16,train_padded,training_labels,validation_padded,validation_labels)
print("With AdaGrad : \n")
mod17=model_eval(model17,train_padded,training_labels,validation_padded,validation_labels)
print("With SGD : \n")
mod18=model_eval(model18,train_padded,training_labels,validation_padded,validation_labels)
print("With RMSProp : \n")
mod19=model_eval(model19,train_padded,training_labels,validation_padded,validation_labels)
print("With AdaDelta : \n")
mod20=model_eval(model20,train_padded,training_labels,validation_padded,validation_labels)

print("Model 5 - CNN 1-D + Bi Directional LSTM \n")

print("With Adam : \n")
mod21=model_eval(model21,train_padded,training_labels,validation_padded,validation_labels)
print("With AdaGrad : \n")
mod22=model_eval(model22,train_padded,training_labels,validation_padded,validation_labels)
print("With SGD : \n")
mod23=model_eval(model23,train_padded,training_labels,validation_padded,validation_labels)
print("With RMSProp : \n")
mod24=model_eval(model24,train_padded,training_labels,validation_padded,validation_labels)
print("With AdaDelta : \n")
mod25=model_eval(model25,train_padded,training_labels,validation_padded,validation_labels)

table = PrettyTable()

table.field_names = ['Model [With Optimiser]', 'Accuracy','Time Taken To Train (secs)']

table.add_row(['CNN-1D [Adam]',round(mod1[1][1]*100,2),round(t1,2)])
table.add_row(['CNN-1D [AdaGrad]', round(mod2[1][1]*100,2),round(t2,2)])
table.add_row(['CNN-1D [SGD]', round(mod3[1][1]*100,2),round(t3,2)])
table.add_row(['CNN-1D [RMSProp]', round(mod4[1][1]*100,2),round(t4,2)])
table.add_row(['CNN-1D [AdaDelta]', round(mod5[1][1]*100,2),round(t5,2)])

table.add_row(['LSTM [Adam]',round(mod6[1][1]*100,2),round(t6,2)])
table.add_row(['LSTM [AdaGrad]', round(mod7[1][1]*100,2),round(t7,2)])
table.add_row(['LSTM [SGD]', round(mod8[1][1]*100,2),round(t8,2)])
table.add_row(['LSTM [RMSProp]', round(mod9[1][1]*100,2),round(t9,2)])
table.add_row(['LSTM [AdaDelta]', round(mod10[1][1]*100,2),round(t10,2)])



table.add_row(['Bidirectional LSTM [Adam]',round(mod11[1][1]*100,2),round(t11,2)])
table.add_row(['Bidirectional LSTM [AdaGrad]', round(mod12[1][1]*100,2),round(t12,2)])
table.add_row(['Bidirectional LSTM [SGD]', round(mod13[1][1]*100,2),round(t13,2)])
table.add_row(['Bidirectional LSTM [RMSProp]', round(mod14[1][1]*100,2),round(t14,2)])
table.add_row(['Bidirectional LSTM [AdaDelta]', round(mod15[1][1]*100,2),round(t15,2)])



table.add_row([' CNN-1D + LSTM  [Adam]',round(mod16[1][1]*100,2),round(t16,2)])
table.add_row([' CNN-1D + LSTM  [AdaGrad]', round(mod17[1][1]*100,2),round(t17,2)])
table.add_row([' CNN-1D + LSTM  [SGD]', round(mod18[1][1]*100,2),round(t18,2)])
table.add_row([' CNN-1D + LSTM  [RMSProp]', round(mod19[1][1]*100,2),round(t19,2)])
table.add_row([' CNN-1D + LSTM  [AdaDelta]', round(mod20[1][1]*100,2),round(t20,2)])



table.add_row([' CNN-1D + Bidirectional LSTM  [Adam]',round(mod21[1][1]*100,2),round(t21,2)])
table.add_row([' CNN-1D + Bidirectional LSTM  [AdaGrad]', round(mod22[1][1]*100,2),round(t22,2)])
table.add_row([' CNN-1D + Bidirectional LSTM  [SGD]', round(mod23[1][1]*100,2),round(t23,2)])
table.add_row([' CNN-1D + Bidirectional LSTM  [RMSProp]', round(mod24[1][1]*100,2),round(t24,2)])
table.add_row([' CNN-1D + Bidirectional LSTM  [AdaDelta]', round(mod25[1][1]*100,2),round(t25,2)])


print(table)

print(table.get_string(sortby='Accuracy',reversesort=True))

print(table.get_string(sortby='Time Taken To Train (secs)',reversesort=True))

"""#Link For References :

1. https://ruder.io/deep-learning-nlp-best-practices/#:~:text=Optimization%20algorithm%20Adam%20(Kingma%20%26%20Ba,stochastic%20gradient%20descent%20(SGD).
2. https://colah.github.io/posts/2015-08-Understanding-LSTMs/
3. https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/
4. https://www.jeansnyman.com/posts/multi-class-text-classification-with-tensorflow/
"""

